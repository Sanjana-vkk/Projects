{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torchvision\nfrom torchvision.models.detection import maskrcnn_resnet50_fpn\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom os import listdir\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.utils import draw_segmentation_masks\nimport matplotlib.patches as patches","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-02T07:16:49.924415Z","iopub.execute_input":"2021-12-02T07:16:49.924707Z","iopub.status.idle":"2021-12-02T07:16:49.930566Z","shell.execute_reply.started":"2021-12-02T07:16:49.924675Z","shell.execute_reply":"2021-12-02T07:16:49.929848Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"path = \"../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/malignant\"\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n\nclass breast_dataset(torch.utils.data.Dataset):\n\n    def __init__(self,path,transforms=None):\n        self.path = path\n        self.transfrom = transforms\n        self.img_name = []\n        self.mask_name = []\n        for img in listdir(self.path):\n            if img[-8:] == \"mask.png\" :\n                self.mask_name.append(img)\n            elif img[-5:] == \").png\":\n                self.img_name.append(img)\n    def __getitem__(self,idx):\n\n        img_path = os.path.join(path,self.img_name[idx])\n        mask_path = os.path.join(path,self.mask_name[idx])\n        img = cv2.imread(img_path)\n        img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2RGB).astype(np.float32)\n        img_nor = img_gray /255\n        img_res = torch.as_tensor(img_nor).to(device).permute(2,0,1)\n        mask = cv2.imread(mask_path)\n        mask_gray = cv2.cvtColor(mask,cv2.COLOR_BGR2GRAY)\n        a,mask_01 = cv2.threshold(mask_gray,127,255,cv2.THRESH_BINARY)\n        contours,hierarchy = cv2.findContours(mask_01,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n        x,y,w,h = cv2.boundingRect((contours[0]))\n        mask_ = mask_01.reshape(1,mask_01.shape[0],mask_01.shape[1])\n        \n        \n        labels = [1]\n        boxes = [x,y,x+w,y+h]\n        target = {}\n        target[\"masks\"] = torch.as_tensor(mask_/255,dtype=torch.int64).to(device)\n        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64).to(device)\n        target[\"boxes\"] = torch.as_tensor([boxes]).to(device)\n        return img_res,target\n\n    def __len__(self):\n        return len(self.img_name)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:16:45.548377Z","iopub.execute_input":"2021-12-02T07:16:45.549377Z","iopub.status.idle":"2021-12-02T07:16:45.567313Z","shell.execute_reply.started":"2021-12-02T07:16:45.549316Z","shell.execute_reply":"2021-12-02T07:16:45.566433Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def model_mask_rcnn(num_class):\n    model = maskrcnn_resnet50_fpn(pretrained=True)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_class)\n    return model\n\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2021-11-23T13:34:49.329773Z","iopub.execute_input":"2021-11-23T13:34:49.330247Z","iopub.status.idle":"2021-11-23T13:34:49.341298Z","shell.execute_reply.started":"2021-11-23T13:34:49.33021Z","shell.execute_reply":"2021-11-23T13:34:49.340515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breast = breast_dataset(path)\ndata_load = torch.utils.data.DataLoader(\n    breast, batch_size=5, shuffle=True,num_workers=0, collate_fn=collate_fn)\n\nnum = 2\nmodel = model_mask_rcnn(num)\nmodel.to(device)\nnum_epoch = 35\nparam = [param for param in model.parameters() if param.requires_grad]\noptimizer = torch.optim.SGD(param, lr=0.002, momentum=0.9)\nlr_scheduler = None\nfor epoch in range(num_epoch):\n    loss_num = 0\n    model.train()\n    for img, targets in data_load:\n        loss_dic = model(img, targets)\n        loss = sum(loss for loss in loss_dic.values())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        loss_num += loss\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n    print(\"eppch:{},loss:{}\".format(epoch, loss_num))\n","metadata":{"execution":{"iopub.status.busy":"2021-11-23T13:34:49.343401Z","iopub.execute_input":"2021-11-23T13:34:49.343692Z","iopub.status.idle":"2021-11-23T13:56:43.701354Z","shell.execute_reply.started":"2021-11-23T13:34:49.343658Z","shell.execute_reply":"2021-11-23T13:56:43.70033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cpu_device = torch.device(\"cpu\")\nmodel = model.eval()\ntest_path = '../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/malignant/malignant (1).png'\nimg_1 = cv2.imread(test_path)\nimg_2 = torch.as_tensor([img_1/255],dtype=torch.float32).permute(0,3,1,2).cuda()\noutputs = model(img_2)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-23T13:56:43.702731Z","iopub.execute_input":"2021-11-23T13:56:43.703296Z","iopub.status.idle":"2021-11-23T13:56:44.547807Z","shell.execute_reply.started":"2021-11-23T13:56:43.703255Z","shell.execute_reply":"2021-11-23T13:56:44.547089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_nms(orig_prediction, iou_thresh=None):\n    \n    # torchvision returns the indices of the bboxes to keep\n    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n    \n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n    \n    return final_prediction\n\n# function to convert a torchtensor back to PIL image\ndef torch_to_pil(img):\n    return torchtrans.ToPILImage()(img).convert('RGB')\n\n\ndef plot_img_bbox(img, target):\n    # plot the image and bboxes\n    # Bounding boxes are defined as follows: x-min y-min width height\n    fig, a = plt.subplots(1,1)\n    fig.set_size_inches(5,5)\n    a.imshow(img)\n    for box in (target['boxes']):\n        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n        rect = patches.Rectangle((x, y),\n                                 width, height,\n                                 linewidth = 2,\n                                 edgecolor = 'r',\n                                 facecolor = 'none')\n\n        # Draw the bounding box on top of the image\n        a.add_patch(rect)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T13:56:44.549068Z","iopub.execute_input":"2021-11-23T13:56:44.549319Z","iopub.status.idle":"2021-11-23T13:56:44.558779Z","shell.execute_reply.started":"2021-11-23T13:56:44.549286Z","shell.execute_reply":"2021-11-23T13:56:44.558017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = outputs[0][\"masks\"][0] > 0.5\nimg_1 = cv2.imread(test_path)\nimg_1 = cv2.cvtColor(img_1,cv2.COLOR_BGR2RGB)\nimg_2 = torch.as_tensor(img_1).permute(2,0,1)\ndog_mask = draw_segmentation_masks(img_2,masks=mask,alpha=0.9)\nout = np.array(dog_mask.permute(1,2,0))\nplt.subplot(1,4,1)\nplt.imshow(out)\nplt.subplot(1,4,2)\nplt.imshow(np.array(mask.detach().permute(1,2,0).to(torch.device(\"cpu\"))).squeeze())\nplt.subplot(1,4,3)\nplt.imshow(cv2.imread(\"../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/malignant/malignant (1)_mask.png\"))\nplt.subplot(1,4,4)\nplt.imshow(img_1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T13:56:44.561413Z","iopub.execute_input":"2021-11-23T13:56:44.561692Z","iopub.status.idle":"2021-11-23T13:56:45.040618Z","shell.execute_reply.started":"2021-11-23T13:56:44.561657Z","shell.execute_reply":"2021-11-23T13:56:45.039989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms as torchtrans\nnms_prediction = apply_nms(outputs[0], iou_thresh=0.1)\nplot_img_bbox(torch_to_pil(img_2), nms_prediction)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T13:56:45.044877Z","iopub.execute_input":"2021-11-23T13:56:45.045525Z","iopub.status.idle":"2021-11-23T13:56:45.306932Z","shell.execute_reply.started":"2021-11-23T13:56:45.045368Z","shell.execute_reply":"2021-11-23T13:56:45.30611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}